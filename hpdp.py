# -*- coding: utf-8 -*-
"""HPDP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nwb61L59tYSdwGNhEPfHwb-I7kNA5Uyj

## HealthCare Prediction on Diabetes Patients

Import the Required Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import zscore
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score
from sklearn.metrics import accuracy_score,mean_squared_error,classification_report,confusion_matrix,precision_score,recall_score,roc_curve,auc
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier

"""Reading and Exploring the HealthCare DataSet"""

dataset=pd.read_csv('health_care_diabetes.csv')
dataset.head()

#Number of Rows and Columns in the Dataset
dataset.shape

# Dataset Information Overview
dataset.info()

# Summary Statistics for the Diabetes Dataset
dataset.describe()

"""Mean of the Features"""

# Identify the mean of the Features
print("Mean of the Pregnancies: ",dataset['Pregnancies'].mean())
print("Mean of the Glucose: ",dataset['Glucose'].mean())
print("Mean of the BloodPressure: ",dataset['BloodPressure'].mean())
print("Mean of the SkinThickness: ",dataset['SkinThickness'].mean())
print("Mean of the Insulin: ",dataset['Insulin'].mean())
print("Mean of the BMI: ",dataset['BMI'].mean())
print("Mean of the DiabetesPedigreeFunction: ",dataset['DiabetesPedigreeFunction'].mean())
print("Mean of the Age: ",dataset['Age'].mean())
print("Mean of the OutCome:",dataset['Outcome'].mean())

#Identify the Number of Rows which has the NULL Values
print('Pregnancies- ',len(dataset['Pregnancies'][dataset['Pregnancies']==0]))
print('Glucose-',len(dataset['Glucose'][dataset['Glucose']==0]))
print('BloodPressure-',len(dataset['BloodPressure'][dataset['BloodPressure']==0]))
print('SkinThickness-',len(dataset['SkinThickness'][dataset['SkinThickness']==0]))
print('Insulin-',len(dataset['Insulin'][dataset['Insulin']==0]))
print('BMI-',len(dataset['BMI'][dataset['BMI']==0]))
print('DiabetesPedigreeFunction-',len(dataset['DiabetesPedigreeFunction'][dataset['DiabetesPedigreeFunction']==0]))
print('Age-',len(dataset['Age'][dataset['Age']==0]))
print('Outcome-',len(dataset['Outcome'][dataset['Outcome']==0]))

#Findings the NULL Value percentage
Columns=['Glucose','BloodPressure','SkinThickness','Insulin','BMI']
null_percentage=(dataset[Columns]==0).mean()*100
print("Percentage of Null Values for Each Column:")
print(null_percentage)

#Replacing the Null Values with the mean values

dataset['Glucose']=dataset['Glucose'].replace([0],[dataset['Glucose'].mean()])
print('Glucose-',len(dataset['Glucose'][dataset['Glucose']==0]))

dataset['BloodPressure']=dataset['BloodPressure'].replace([0],[dataset['BloodPressure'].mean()])
print('BloodPressure-',len(dataset['BloodPressure'][dataset['BloodPressure']==0]))

dataset['SkinThickness']=dataset['SkinThickness'].replace([0],[dataset['SkinThickness'].mean()])
print('SkinThickness-',len(dataset['SkinThickness'][dataset['SkinThickness']==0]))

dataset['Insulin']=dataset['Insulin'].replace([0],[dataset['Insulin'].mean()])
print('Insulin-',len(dataset['Insulin'][dataset['Insulin']==0]))

dataset['BMI']=dataset['BMI'].replace([0],[dataset['BMI'].mean()])
print('BMI-',len(dataset['BMI'][dataset['BMI']==0]))

#Checking the null value percentage of the treated columns
null_percentage_treated = (dataset[Columns] == 0).mean() * 100

# Displaying the null value percentage for each selected column
print("Percentage of Null Values for Each Column after the null value treatment:")
print(null_percentage_treated)

"""Outliers Detection and Treatment"""

Column=['Glucose','BloodPressure','SkinThickness','Insulin','BMI']

#Detecting outliers and Treatment
plt.figure(figsize=(12,8))
sns.boxplot(data=dataset[Column])
plt.title("Boxplot for Numeric Columns")
plt.show()

dataset.describe()

"""Finding the Outliers count in selected columns:"""

def find_Outliers_IQR(dataset,column_name):

  Q1 = dataset[column_name].quantile(0.25)
  Q2 = dataset[column_name].quantile(0.75)

  IQR= Q2-Q1

  lower_bound= Q1-1.5*IQR
  upper_bound= Q2+1.5*IQR

  #Find Outliers
  outliers = dataset[(dataset[column_name] < lower_bound) | (dataset[column_name] > upper_bound)]
  count_outliers=len(outliers)
  return count_outliers

for column_name in Columns:
   outlier_count=find_Outliers_IQR(dataset,column_name)
   print(f"Number of Outliers in the '{column_name}' Column:{outlier_count}")

"""Outlier Treatment:"""

sorted(dataset)
Q1 = dataset.quantile(0.20)
Q2 = dataset.quantile(0.80)
IQR = Q2 - Q1
print(IQR)

data_cleared_iqr = dataset[~((dataset < (Q1 - 1.5 * IQR)) | (dataset > (Q2 + 1.5*IQR))).any(axis=1)]
data_cleared_iqr
print(data_cleared_iqr.shape)
print(dataset.shape)

data_cleared_iqr

col=data_cleared_iqr[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']]

"""Checking the Outliers after treatment using boxplot

"""

col=data_cleared_iqr[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']]
plt.figure(figsize=(12,8))
sns.boxplot(data=col)
plt.show()

"""**Using Z-Scores to Detect Number of Outliers**"""

z_scores = zscore(dataset.select_dtypes(include=['float', 'int']))
data_outliers = dataset[(z_scores > 3).any(axis=1)]
print(f"Number of Outliers: {len(data_outliers)}")

"""EDA-Exploratory Data Analysis"""

dataset['Glucose'].plot(kind='hist',figsize=(10,5))
plt.legend()
plt.show()

dataset['BloodPressure'].plot(kind='hist',figsize=(10,5))
plt.legend()
plt.show()

dataset['SkinThickness'].plot(kind='hist',figsize=(10,5))
plt.legend()
plt.show()

dataset['Insulin'].plot(kind='hist',figsize=(10,5))
plt.legend()
plt.show()

dataset['BMI'].plot(kind='hist',figsize=(10,5))
plt.legend()
plt.show()

"""Violin Plot for Important Features

"""

plt.figure(figsize=(15,10))
sns.violinplot(data=dataset[Columns])
plt.title("Violin Plot for Selected Features")
plt.show()

"""Kernel Density For Important Features"""

plt.figure(figsize=(15, 10))
for column in Columns:
    sns.kdeplot(dataset[column], label=column)
plt.title("Kernel Density Estimation (KDE) Plot of Numeric Features")
plt.legend()
plt.show()

dataset.dtypes

print(dataset.dtypes.value_counts())


figsize=(16,2)
dataset.dtypes.value_counts().plot(kind='barh')
plt.legend()
plt.show()

dataset['Outcome'].value_counts()

dataset['Outcome'].value_counts().plot(kind='bar')
plt.legend()
plt.title("Outcome")
plt.show()

outcome=(dataset['Outcome'].value_counts()/dataset['Outcome'].shape)*100
outcome

balanced_data=100-outcome
balanced_data

balanced_data.plot(kind='bar')
plt.title("Balanced Data")
plt.legend()
plt.show()

"""Bi-Variate Analysis

Creating scatter charts between the pair of variables to understand the relationships.
"""

#Bi-Variate Analysis

plt.figure(figsize=(12,5))
sns.scatterplot(x='Pregnancies',y='Glucose',hue='Outcome',data=dataset)
plt.show()

plt.figure(figsize=(12,5))
sns.scatterplot(x='Glucose',y='BloodPressure',hue='Outcome',data=dataset)
plt.show()

plt.figure(figsize=(12,5))
sns.scatterplot(x='BloodPressure',y='SkinThickness',hue='Outcome',data=dataset)
plt.show()

sns.pairplot(dataset)
plt.suptitle("Pairplot of Numeric Features",y=1)
plt.show()

### Multi-Variate Analysis
### Perform correlation analysis. Visually explore it using a heat map.

plt.figure(figsize=(10,7))
sns.heatmap(dataset.corr(),annot=True)
plt.show()

"""Data Modeling:"""

#Data Preparation for Modeling:
x=x=dataset.drop(['Outcome'],axis=1)
y=dataset.Outcome

# Finding the Correlation of every feature with the Outcome (Target Variable
dataset.corrwith(dataset['Outcome'])

bestfeatures = SelectKBest(score_func=chi2, k='all')
fit = bestfeatures.fit(x,y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(x.columns)
#concat two dataframes for better visualization
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns
print(featureScores.nlargest(8,'Score'))

plt.figure(figsize=(10,7))
sns.heatmap(dataset.corr(),annot=True)
plt.show()

"""Tree-based Classifier

"""

model = ExtraTreesClassifier()
model.fit(x,y)
print(model.feature_importances_)
print('\n')
#use inbuilt class feature_importances of tree based classifiers
#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=x.columns)
feat_importances.nlargest(8).plot(kind='barh')
plt.show()
print('\n')
feat_importances.nlargest(8).plot(kind='pie')
plt.show()

new_x=dataset.drop(['Outcome','BloodPressure'],axis=1).values
new_y=dataset.Outcome.values

"""SMOTE to address the Class Imbalance

Train a Model
"""

trainx,testx,trainy,testy=train_test_split(new_x,new_y,test_size=0.20,random_state=10)

print("Before OverSampling, counts of label '1': {}".format(sum(trainy == 1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(trainy == 0)))

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state =63)
trainx_res,trainy_res = sm.fit_resample(trainx,trainy.ravel())
print('After OverSampling, the shape of train_X: {}'.format(trainx_res.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(trainy_res.shape))


print("After OverSampling, counts of label '1': {}".format(sum(trainy_res == 1)))
print("After OverSampling, counts of label '0': {}".format(sum(trainy_res == 0)))

sc=StandardScaler()
trainx_scaled=sc.fit_transform(trainx)
testx_scaled=sc.fit_transform(testx)

"""Applying an appropraite classification algorithm to build a Model

Model 1: Building a Logistic Regression Model
"""

LogisticReg=LogisticRegression(solver='liblinear',random_state=123)

LogisticReg.fit(trainx_res,trainy_res)

Prediction=LogisticReg.predict(testx)

print('Accuracy_Score - ',accuracy_score(testy,Prediction))
print('Mean_Squared_Error - ',mean_squared_error(testy,Prediction))

print((confusion_matrix(testy,Prediction)))

print(classification_report(testy,Prediction,zero_division=0))

#Preparing ROC Curve (Receiver Operating Characteristics Curve)
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# predict probabilities
probs = LogisticReg.predict_proba(trainx_res)
# keep probabilities for the positive outcome only
probs = probs[:, 1]
# calculate AUC
auc = roc_auc_score(trainy_res, probs)
print('AUC: %.3f' % auc)
# calculate roc curve
fpr, tpr, thresholds = roc_curve(trainy_res, probs)
# plot no skill
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.')
plt.show()

"""Model 2: Random Forest Classifier"""

rf=RandomForestClassifier(random_state=42,max_depth=5)

rf.fit(trainx_res,trainy_res)

rf_predict=rf.predict(testx)

print('Accuracy_score -',accuracy_score(testy,rf_predict))
print('Mean_squared_error -',mean_squared_error(testy,rf_predict))

# RandomForestClassifier( Hyper Parameter Tuning )

param_grid={'n_estimators':[100,400,200,300],'criterion':['gini','entropy'],'max_depth':[1,2,3],'min_samples_split':[2,4,3],'min_samples_leaf':[1,2,3],
'max_leaf_nodes':[1,2,3],'max_samples':[2,4,3]}

grid=GridSearchCV( estimator=rf,param_grid=param_grid,n_jobs=-1,cv=5,verbose=2)

rf_grid=RandomForestClassifier(criterion= 'gini',max_depth= 2,max_leaf_nodes=3,max_samples=4,min_samples_leaf= 1,min_samples_split=3,
 n_estimators= 400,random_state=42)

rf_grid.fit(trainx_res,trainy_res)

rf_grid_predict=rf_grid.predict(testx)

print('Accuracy_score -',accuracy_score(testy,rf_grid_predict))
print('Mean_squared_error -',mean_squared_error(testy,rf_grid_predict))

print((confusion_matrix(testy,Prediction)))

print(classification_report(testy,Prediction))

#Preparing ROC Curve (Receiver Operating Characteristics Curve)
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# predict probabilities
probs = rf.predict_proba(trainx_res)
# keep probabilities for the positive outcome only
probs = probs[:, 1]
# calculate AUC
auc = roc_auc_score(trainy_res, probs)
print('AUC: %.3f' % auc)
# calculate roc curve
fpr, tpr, thresholds = roc_curve(trainy_res, probs)
# plot no skill
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.')
plt.show()

"""Model 3: Decision Tree Classifier"""

dc=DecisionTreeClassifier(random_state=42)

dc.fit(trainx_res,trainy_res)

dc_pred=dc.predict(testx)

print('Accuracy_score -',accuracy_score(testy,dc_pred))
print('Mean_squared_error -',mean_squared_error(testy,dc_pred))

### DecisionTreeClassifier( Hyper Parameter Tunning )

dc_param_grid={'splitter':["best", "random"],'criterion':['gini','entropy'],'max_depth':[1,2,3],
'min_samples_split':[1,2,3],'min_samples_leaf':[1,2,3],'max_leaf_nodes':[1,2,3]}

import warnings
warnings.filterwarnings('ignore')
dc_grid=GridSearchCV(estimator=dc,param_grid=dc_param_grid,n_jobs=-1,cv=5,verbose=2)
dc_grid.fit(trainx_res,trainy_res)

dc_grid.best_params_

dc_final=DecisionTreeClassifier(criterion= 'gini', max_depth=2,max_leaf_nodes=4,min_samples_leaf= 1,
min_samples_split= 2,splitter='best',random_state=42)

dc_final.fit(trainx_res,trainy_res)
dc_final_pred=dc_final.predict(testx)

print('Accuracy_score -',accuracy_score(testy,dc_final_pred))
print('Mean_squared_error -',mean_squared_error(testy,dc_final_pred))

print((confusion_matrix(testy,dc_final_pred)))

print((classification_report(testy,dc_final_pred)))

#Preparing ROC Curve (Receiver Operating Characteristics Curve)
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# predict probabilities
probs = dc_final.predict_proba(trainx_res)
# keep probabilities for the positive outcome only
probs = probs[:, 1]
# calculate AUC
auc = roc_auc_score(trainy_res, probs)
print('AUC: %.3f' % auc)
# calculate roc curve
fpr, tpr, thresholds = roc_curve(trainy_res, probs)
# plot no skill
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.')
plt.show()

Algorithms=['RandomForest','Decisiontree']
Accuracy_Score=[accuracy_score(testy,rf_grid_predict),accuracy_score(testy,dc_final_pred)]
# Create a DataFrame
accuracy_df = pd.DataFrame({'Algorithm': Algorithms, 'Accuracy': Accuracy_Score})

# Display the accuracy table
print(accuracy_df)

"""Model 4: Gradient Boosting Classifier"""

from sklearn.ensemble import GradientBoostingClassifier

# Train Gradient Boosting Classifier with Hyperparameter Tuning
param_grid_gb = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [2, 3, 5]
}

grid_gb = GridSearchCV(GradientBoostingClassifier(), param_grid_gb, cv=5, scoring='accuracy', n_jobs=-1)
grid_gb.fit(trainx_res,trainy_res)

# Best Gradient Boosting Model
best_gb = grid_gb.best_estimator_
y_pred_gb = best_gb.predict(testx)

# Model Evaluation
print("Gradient Boosting Classifier Performance:")
print(classification_report(testy, y_pred_gb))
print(f"Accuracy: {accuracy_score(testy, y_pred_gb):.4f}")
print(f"Mean Squared Error: {mean_squared_error(testy, y_pred_gb):.4f}")

# ROC Curve for Gradient Boosting
probs_gb = best_gb.predict_proba(testx)[:, 1]
fpr_gb, tpr_gb, _ = roc_curve(testy, probs_gb)
auc_gb = roc_auc_score(testy, probs_gb)

plt.plot(fpr_gb, tpr_gb, marker='.', label=f'Gradient Boosting (AUC = {auc_gb:.3f})')

gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb_model.fit(trainx_res,trainy_res)

# Predictions
y_pred_gb = gb_model.predict(testx)

# Model Evaluation
print("Gradient Boosting Classifier Performance:")
print(classification_report(testy, y_pred_gb))
print(f"Accuracy: {accuracy_score(testy, y_pred_gb):.4f}")

#Preparing ROC Curve (Receiver Operating Characteristics Curve)
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# predict probabilities
probs = gb_model.predict_proba(trainx_res)
# keep probabilities for the positive outcome only
probs = probs[:, 1]
# calculate AUC
auc = roc_auc_score(trainy_res, probs)
print('AUC: %.3f' % auc)
# calculate roc curve
fpr, tpr, thresholds = roc_curve(trainy_res, probs)
# plot no skill
plt.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
plt.plot(fpr, tpr, marker='.')
plt.show()

"""Model 5: Support Vector Machine (SVM)"""

from sklearn.svm import SVC

# Train SVM Classifier with Hyperparameter Tuning
param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

grid_svm = GridSearchCV(SVC(probability=True), param_grid_svm, cv=5, scoring='accuracy', n_jobs=-1)
grid_svm.fit(trainx_scaled, trainy)

# Best SVM Model
best_svm = grid_svm.best_estimator_
y_pred_svm = best_svm.predict(testx_scaled)

# Model Evaluation
print("SVM Classifier Performance:")
print(classification_report(testy, y_pred_svm))
print(f"Accuracy: {accuracy_score(testy, y_pred_svm):.4f}")
print(f"Mean Squared Error: {mean_squared_error(testy, y_pred_svm):.4f}")

# ROC Curve for SVM
probs_svm = best_svm.predict_proba(testx_scaled)[:, 1]
fpr_svm, tpr_svm, _ = roc_curve(testy, probs_svm)
auc_svm = roc_auc_score(testy, probs_svm)

plt.plot(fpr_svm, tpr_svm, marker='.', label=f'SVM (AUC = {auc_svm:.3f})')

# Train SVM Classifier
svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)
svm_model.fit(trainx_scaled,trainy)

# Predictions
y_pred_svm = svm_model.predict(testx_scaled)

# Model Evaluation
print("Support Vector Machine (SVM) Performance:")
print(classification_report(testy, y_pred_svm))
print(f"Accuracy: {accuracy_score(testy, y_pred_svm):.4f}")

# Predict probabilities for SVM Model
svm_probs = svm_model.predict_proba(testx_scaled)[:, 1]  # Only positive class probabilities

# Compute AUC Score
auc_svm = roc_auc_score(testy, svm_probs)
print(f'AUC Score for SVM: {auc_svm:.3f}')

# Compute ROC Curve
fpr_svm, tpr_svm, _ = roc_curve(testy, svm_probs)

# Plot the ROC Curve
plt.figure(figsize=(8, 6))
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='No Skill')
plt.plot(fpr_svm, tpr_svm, marker='.', label=f'SVM (AUC = {auc_svm:.3f})', color='blue')

# Labels and Title
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - SVM Model')
plt.legend()
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

# Define function to evaluate models
def evaluate_model(model, test_x, test_y):
    predictions = model.predict(test_x)
    accuracy = accuracy_score(test_y, predictions)
    precision = precision_score(test_y, predictions)
    recall = recall_score(test_y, predictions)
    f1 = f1_score(test_y, predictions)

    print(f"Model: {model.__class__.__name__}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(confusion_matrix(test_y, predictions))
    print(classification_report(test_y, predictions))
    print("\n")

# Model 1: Logistic Regression
logreg = LogisticRegression(solver='liblinear', random_state=123)
logreg.fit(trainx_res, trainy_res)
evaluate_model(logreg, testx, testy)

# Model 2: Random Forest Classifier
rf_grid.fit(trainx_res, trainy_res)
evaluate_model(rf_grid, testx, testy)

# Model 3: Decision Tree Classifier
dc_final.fit(trainx_res, trainy_res)
evaluate_model(dc_final, testx, testy)

# Model 4: KNeighbors Classifier (as an additional model)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(trainx_res, trainy_res)
evaluate_model(knn, testx, testy)

# Optional: Hyperparameter tuning for KNeighbors Classifier
param_grid_knn = {'n_neighbors': [3, 5, 7, 9]}
grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, n_jobs=-1)
grid_knn.fit(trainx_res, trainy_res)
best_knn = grid_knn.best_estimator_
print("Best KNeighbors Classifier:", best_knn)
evaluate_model(best_knn, testx, testy)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Define a function to evaluate and store results
def evaluate_and_store(model, test_x, test_y, results):
    predictions = model.predict(test_x)
    accuracy = accuracy_score(test_y, predictions)
    precision = precision_score(test_y, predictions)
    recall = recall_score(test_y, predictions)
    f1 = f1_score(test_y, predictions)

    results.append({
        'Model': model.__class__.__name__,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    })

# List to store results
results = []

# Evaluate models
evaluate_and_store(logreg, testx, testy, results)
evaluate_and_store(rf_grid, testx, testy, results)
evaluate_and_store(dc_final, testx, testy, results)
evaluate_and_store(knn, testx, testy, results)
evaluate_and_store(gb_model,testx,testy,results)
evaluate_and_store(svm_model,testx_scaled,testy,results)
#evaluate_and_store(best_gb_model, testx, testy, results)

# Convert results to DataFrame for easy visualization
results_df = pd.DataFrame(results)

# Display results
print(results_df)

# Step 4: Visualizing results
results_df.set_index('Model').plot(kind='bar', figsize=(10, 6))
plt.title('Model Comparison')
plt.ylabel('Scores')
plt.xticks(rotation=45)
plt.legend(loc='best')
plt.show()

try:
    import sweetviz
except:
    !pip install sweetviz --user
    print('Restart runtime to import sweetviz')
    import sweetviz
import pandas as pd
df = pd.read_csv('health_care_diabetes.csv')
analysis = sweetviz.analyze(df)
analysis.show_notebook() # or export with show_html()